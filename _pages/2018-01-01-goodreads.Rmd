---
title: "Untitled"
output: github_document
---

# Good Reads: Recommender System

![title](https://blog.tellwell.ca/wp-content/uploads/2016/12/goodreads.jpg)

## Overview

We cannot escape! If you use online services or buy anything from a e-commerce company, recommendation is part of your online routine. Online services are suggesting you products and services that you might like. It is everywhere. Netflix recommends shows/movies based on what you watched and recent demand. Amazon displays on your website products that you might be interested based on previous purchases, clicks, and user behavior. Youtube recommends videos and channels based on what you and others have watched. We could keep going and the idea would be the same: suggest something that you probably will enjoy.

Books are not different!! The website [goodreads.com](http://www.goodreads.com) could be defined as a social network dedicated for people interested on books. The idea is that you can interact with other readers, authors, and of course books. Between many features, the readers social network provides a members review database to give you more information for the books you are looking for. It works excatly as any other review system. You rate the book you have read (between 1 and 5 stars) and write down your opinion about it, explaining what you like or don't. I know, nothing is new here. The [goodreads.com](http://www.goodreads.com) also have its recommender system for books. It will look for the books you read and the rates you gave to suggest you new books.

In this notebook, I apply a very well known called **Item-Based Collaborative Filtering** (IBCF) technique to estimate the rate of books based on other similar books. The idea is simple, if I like a book (i.e., I rated it 5 stars) it is likely that I will also enjoy a very similar book to that one.

This technique is not new and also there are other methodologies to estimate the rates. Here we are going to focus on the IBCF and see how it works when predicting books rating!!!

## Loading and Cleaning data


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load necessary libraries
library(dplyr)
library(ggplot2)
library(tidyr)
```

The data is available at [Kaggle](https://www.kaggle.com/zygmunt/goodbooks-10k/data). Here we find a dataset with millions of ratings for 10k books. In this page we can also find a very good kernel about **User-Based Collaborative Filtering** (UBCF). The objective is the same, to improve the recommendations system but **UBCF** approach looks for similar users instead of books.

```{r}
ratings<-read.csv("~/project/Data-Science-Projects/goodreads/data/ratings.csv")
books<-read.csv("~/project/Data-Science-Projects/goodreads/data/books.csv")
```

For this application we are going to use 2 datasets:

* **ratings**: Contains information about ratings. It is a simple dataset with only 3 variables: **book_id**, **user_id**, and **rating**. It is pretty straightforward. It represents the rate given from a user for a book.
* **books**: This is dataset has more information. It contains different features for books such as **title**, **author name**, **year of publication**, **number of reviews**, and etc. 

```{r}
knitr::kable(
  head(books)
)
```

```{r}
knitr::kable(
  head(ratings)
)
```

Let's start to clean our **ratings** dataset. It is possible for a user have more than one rating for the same book. We could assume that the user change its mind and reevalute the book rating. It definitely can happen, but for simplicity we remove these cases and assume that every combination of book and user has only one rating. In this case, we have to eliminate the cases where users gave more than on rating for the same book.

```{r}
ratings<-ratings %>% group_by(user_id, book_id) %>% mutate(total=n())
cat('Number of duplicate ratings: ', format(nrow(ratings[ratings$total > 1,]),big.mark=",",scientific=FALSE))
```

Example of duplicated rating: As you can see below, user **3204** rated book **8946** five times.

```{r}
duplicated_ratings<-ratings%>%
    group_by(book_id,user_id)%>%
    summarise(total=n())%>%
    filter(total>1)%>%
    arrange(desc(total))

head(duplicated_ratings)
```

The duplicated cases were removed.

```{r}
ratings <- ratings[ratings$total == 1,]
cat(' Number of ratings: ',format(nrow(ratings),big.mark=",",scientific=FALSE),'\n',
    'Number of Books: ',format(length(unique(ratings$book_id)),big.mark=",",scientific=FALSE),'\n',
    'Number of Users: ',format(length(unique(ratings$user_id)),big.mark=",",scientific=FALSE))
```

For this problem, I decided to work with books that have atleast 100 reviews, and user who gave more than 20 reviews. Doing that I am significantly reducing reducing the number books and users. It helped me to reduce process time consumption.

```{r}
ratings<-ratings%>%group_by(user_id)%>%mutate(total=n())%>%filter(total>20)
ratings<-ratings%>%group_by(book_id)%>%mutate(total=n())%>%filter(total==100)
cat(' Number of ratings: ',format(nrow(ratings),big.mark=",",scientific=FALSE),'\n',
    'Number of Books: ',format(length(unique(ratings$book_id)),big.mark=",",scientific=FALSE),'\n',
    'Number of Users: ',format(length(unique(ratings$user_id)),big.mark=",",scientific=FALSE))
```

Now, we have 9,806,160 combinatations of (**book**, **user**), and only 145,600 ratings. It means that less than 2% of these possible combination have been rated.

## EDA: Understanding more about our data

### What's the most reviewed book?

